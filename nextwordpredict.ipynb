{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WikiText-103 Next-Word Prediction Model\n",
    "# Complete implementation for Google Colab\n",
    "\n",
    "# Install required packages\n",
    "!pip install transformers tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe967f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\" Loading WikiText-103 dataset...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Data preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special tokens and formatting\n",
    "    text = re.sub(r'@-@', '-', text)  # WikiText specific token\n",
    "    text = re.sub(r'= = .+ = =', '', text)  # Headers\n",
    "    text = re.sub(r'= .+ =', '', text)  # Subheaders\n",
    "\n",
    "    # Keep only letters, numbers, spaces, and basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?\\;\\:]', ' ', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def create_sequences_and_targets(texts, tokenizer, seq_length=5):\n",
    "    \"\"\"Create input sequences and target words for training\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    for text in texts:\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            continue\n",
    "\n",
    "        # Tokenize text\n",
    "        tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "        # Create sequences of length seq_length + 1 (input + target)\n",
    "        for i in range(len(tokens) - seq_length):\n",
    "            sequence = tokens[i:i + seq_length + 1]\n",
    "            if len(sequence) == seq_length + 1:\n",
    "                sequences.append(sequence[:-1])  # Input sequence\n",
    "                targets.append(sequence[-1])     # Target word\n",
    "\n",
    "    return np.array(sequences), np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "print(\" Preprocessing training data...\")\n",
    "train_texts = []\n",
    "for example in dataset['train']:\n",
    "    cleaned = clean_text(example['text'])\n",
    "    if cleaned and len(cleaned.split()) > 10:  # Only keep sentences with >10 words\n",
    "        train_texts.append(cleaned)\n",
    "\n",
    "# Limit dataset size for faster training in Colab\n",
    "print(f\"Total training examples: {len(train_texts)}\")\n",
    "train_texts = train_texts[:50000]  # Use first 50k examples\n",
    "print(f\"Using {len(train_texts)} training examples for faster training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ae3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit tokenizer\n",
    "print(\" Creating tokenizer...\")\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create training sequences\n",
    "print(\" Creating training sequences...\")\n",
    "seq_length = 5  # Predict next word based on previous 5 words\n",
    "X_train, y_train = create_sequences_and_targets(train_texts, tokenizer, seq_length)\n",
    "\n",
    "print(f\"Training sequences shape: {X_train.shape}\")\n",
    "print(f\"Training targets shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert targets to categorical (one-hot encoding)\n",
    "y_train_categorical = to_categorical(y_train, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb941601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build LSTM model\n",
    "print(\" Building LSTM model...\")\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=seq_length),\n",
    "    LSTM(150, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
    "    LSTM(150, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ca3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "print(\" Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train_categorical,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model and tokenizer\n",
    "print(\" Saving model and tokenizer...\")\n",
    "model.save('next_word_model.h5')\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\" Model and tokenizer saved!\")\n",
    "\n",
    "# Prediction functions\n",
    "def predict_next_word(input_text, model=model, tokenizer=tokenizer, seq_length=5, top_k=3):\n",
    "    \"\"\"\n",
    "    Predict the next word given an input text\n",
    "\n",
    "    Args:\n",
    "        input_text: String of input text\n",
    "        model: Trained Keras model\n",
    "        tokenizer: Fitted tokenizer\n",
    "        seq_length: Length of input sequence\n",
    "        top_k: Number of top predictions to return\n",
    "\n",
    "    Returns:\n",
    "        List of (word, probability) tuples\n",
    "    \"\"\"\n",
    "    # Clean and tokenize input\n",
    "    cleaned_text = clean_text(input_text)\n",
    "    tokens = tokenizer.texts_to_sequences([cleaned_text])[0]\n",
    "\n",
    "    # Take last seq_length tokens\n",
    "    if len(tokens) >= seq_length:\n",
    "        input_sequence = tokens[-seq_length:]\n",
    "    else:\n",
    "        # Pad with zeros if input is shorter than seq_length\n",
    "        input_sequence = [0] * (seq_length - len(tokens)) + tokens\n",
    "\n",
    "    # Reshape for prediction\n",
    "    input_sequence = np.array([input_sequence])\n",
    "\n",
    "    # Get prediction probabilities\n",
    "    predictions = model.predict(input_sequence, verbose=0)[0]\n",
    "\n",
    "    # Get top k predictions\n",
    "    top_indices = np.argsort(predictions)[-top_k:][::-1]\n",
    "\n",
    "    # Convert indices back to words\n",
    "    word_predictions = []\n",
    "    for idx in top_indices:\n",
    "        if idx in tokenizer.index_word:\n",
    "            word = tokenizer.index_word[idx]\n",
    "            probability = predictions[idx]\n",
    "            word_predictions.append((word, probability))\n",
    "\n",
    "    return word_predictions\n",
    "\n",
    "def predict_next_word_simple(input_text):\n",
    "    \"\"\"Simple function that returns just the most likely next word\"\"\"\n",
    "    predictions = predict_next_word(input_text, top_k=1)\n",
    "    if predictions:\n",
    "        return predictions[0][0]\n",
    "    return \"<unknown>\"\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nðŸŽ¯ Testing predictions...\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"once upon a time in\",\n",
    "    \"the quick brown fox\",\n",
    "    \"artificial intelligence is\",\n",
    "    \"machine learning algorithms\",\n",
    "    \"deep learning models can\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predictions = predict_next_word(sentence, top_k=3)\n",
    "    print(f\"\\nInput: '{sentence}'\")\n",
    "    print(\"Top 3 predictions:\")\n",
    "    for i, (word, prob) in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {word} (probability: {prob:.4f})\")\n",
    "\n",
    "# Interactive prediction function\n",
    "def interactive_prediction():\n",
    "    \"\"\"Interactive function for testing predictions\"\"\"\n",
    "    print(\"\\nðŸŽ® Interactive Next-Word Prediction\")\n",
    "    print(\"Enter a sentence and get next word predictions!\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter text: \").strip()\n",
    "\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        if not user_input:\n",
    "            print(\"Please enter some text!\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            predictions = predict_next_word(user_input, top_k=5)\n",
    "            if predictions:\n",
    "                print(f\"\\nNext word predictions for: '{user_input}'\")\n",
    "                for i, (word, prob) in enumerate(predictions, 1):\n",
    "                    print(f\"  {i}. {word} (probability: {prob:.4f})\")\n",
    "            else:\n",
    "                print(\"No predictions available for this input.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error making prediction: {e}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "# Function to load saved model and tokenizer\n",
    "def load_saved_model():\n",
    "    \"\"\"Load previously saved model and tokenizer\"\"\"\n",
    "    try:\n",
    "        loaded_model = load_model('next_word_model.h5')\n",
    "\n",
    "        with open('tokenizer.pickle', 'rb') as f:\n",
    "            loaded_tokenizer = pickle.load(f)\n",
    "\n",
    "        print(\" Model and tokenizer loaded successfully!\")\n",
    "        return loaded_model, loaded_tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading model: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901883fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display final results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" NEXT-WORD PREDICTION MODEL READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"â€¢ predict_next_word(text, top_k=3) - Get top k predictions\")\n",
    "print(\"â€¢ predict_next_word_simple(text) - Get single best prediction\")\n",
    "print(\"â€¢ interactive_prediction() - Interactive testing\")\n",
    "print(\"â€¢ load_saved_model() - Load saved model\")\n",
    "\n",
    "print(f\"\\nModel Info:\")\n",
    "print(f\"â€¢ Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"â€¢ Sequence length: {seq_length}\")\n",
    "print(f\"â€¢ Training examples: {len(X_train):,}\")\n",
    "print(f\"â€¢ Model parameters: {model.count_params():,}\")\n",
    "\n",
    "print(\"\\n Try running: interactive_prediction()\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "print(\" Saving model and tokenizer...\")\n",
    "model.save('next_word_model.h5')\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\" Model and tokenizer saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
